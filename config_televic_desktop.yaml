# name
experiment: "ContAN"

# seed
seed: 20211024

# train params
epochs: 10
batch_size: 16
workers: 1
train_parts: 1
valid_parts: 1
valid_start: 0
save_start: 0

optimizer: Adadelta
lr: 1
lr_decay: cosine
step_ratio: 10
step_decay: 5
eps: 1e-6
weight_decay: 1e-4
beta: 0.9

dropout: True
dropout_ratio: 0.5
relu: True
gradient: 100
gradient_clip: True
use_label_mask: False

# datapaths
train_label_path: 'D:\Masterproef\echmer\data\Televic\mathpix_processed_data\test.txt'
train_image_path: 'NA'

eval_label_path: 'D:\Masterproef\echmer\data\Televic\mathpix_processed_data\val.txt'
eval_image_path: 'NA'

test_label_path: 'D:\Masterproef\echmer\data\Televic\mathpix_processed_data\test.txt'
test_image_path: 'NA'

question_file: 'D:\Masterproef\echmer\data\Televic\formatted_questions.txt'

word_path: 'D:\Masterproef\CAN\MLHME-38K\train_set\words.txt'

# collate_fn
collate_fn: collate_fn2

densenet:
  ratio: 16
  growthRate: 24
  reduction: 0.5
  bottleneck: True
  use_dropout: True

encoder:
  input_channel: 1
  out_channel: 684

decoder:
  net: AttDecoder
  cell: 'GRU'
  input_size: 256
  hidden_size: 256

# out_channel should be equal to the amount of symbols in the dictionary
counting_decoder:
  in_channel: 684
  out_channel: 206

attention:
  attention_dim: 512
  word_conv_kernel: 1

question_context:
  out_channel: 768

attention_map_vis_path: 'vis/attention_map'
counting_map_vis_path: 'vis/counting_map'

whiten_type: None
max_step: 256

optimizer_save: False
finetune: False
checkpoint_dir: 'checkpoints'
checkpoint: 'D:\Masterproef\CAN\checkpoints\CAN_2024-05-29-13-47_decoder-AttDecoder\CAN_2024-05-29-13-47_decoder-AttDecoder_WordRate-0.9196_ExpRate-0.4780_30.pth'
log_dir: 'logs'
